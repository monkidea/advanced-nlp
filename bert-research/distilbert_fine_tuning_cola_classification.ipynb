{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_0aIMt57vbjE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "GqhvUzGovjTq",
    "outputId": "907991f8-a9a2-4496-fcfa-5b8c09439fd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs is : 0\n",
      "Exception occured while checking for GPU support..\n"
     ]
    }
   ],
   "source": [
    "def check_gpu_set_device():\n",
    "    try:\n",
    "        if torch.cuda.is_available:\n",
    "          print ('Number of GPUs is :',torch.cuda.device_count())\n",
    "          print ('Name of GPU is :', torch.cuda.get_device_name())\n",
    "          device = torch.device(\"cuda\")\n",
    "        else:\n",
    "          device = torch.device(\"cpu\")\n",
    "    except : \n",
    "        print ('Exception occured while checking for GPU support..')\n",
    "        device = torch.device(\"cpu\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "device = check_gpu_set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "colab_type": "code",
    "id": "dPJ3aTQLvu1h",
    "outputId": "3036634b-96ea-4139-ceb6-dca1b882f2e1"
   },
   "outputs": [],
   "source": [
    "# install transformers\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "colab_type": "code",
    "id": "xGHpRN6XxzDk",
    "outputId": "66099f47-fcbb-4353-bd1a-90269436130d"
   },
   "outputs": [],
   "source": [
    "# install wget\n",
    "#!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "_k5Y-AS6x4Q5",
    "outputId": "552882d2-f592-4b91-e092-b36b3fcd65b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [............................................................................] 255330 / 255330"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cola_public_1.1 (1).zip'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download cola dataset\n",
    "import wget\n",
    "import os\n",
    "\n",
    "wget.download('https://nyu-mll.github.io/CoLA/cola_public_1.1.zip')\n",
    "#!unzip cola_public_1.1.zip -d ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U9AHplyeyLkh"
   },
   "outputs": [],
   "source": [
    "#load cola dataset in dataframe\n",
    "df = pd.read_csv('./cola_public_1.1/cola_public/raw/in_domain_train.tsv', delimiter='\\t', header=None, names=['A','label','B','sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "colab_type": "code",
    "id": "EM7fBP_3y-AC",
    "outputId": "0a1a0e3a-f3ee-4961-92bb-8f38e10d60b8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "      <th>B</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our friends won't buy this analysis, let alone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One more pseudo generalization and I'm giving up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One more pseudo generalization or I'm giving up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The more we study verbs, the crazier they get.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gj04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Day by day the facts are getting murkier.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A  label    B                                           sentence\n",
       "0  gj04      1  NaN  Our friends won't buy this analysis, let alone...\n",
       "1  gj04      1  NaN  One more pseudo generalization and I'm giving up.\n",
       "2  gj04      1  NaN   One more pseudo generalization or I'm giving up.\n",
       "3  gj04      1  NaN     The more we study verbs, the crazier they get.\n",
       "4  gj04      1  NaN          Day by day the facts are getting murkier."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "92JfSbYFzR66"
   },
   "outputs": [],
   "source": [
    "labels = df.label.values\n",
    "sentences = df.sentence.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "DWHlF66nzVtx",
    "outputId": "1e36dbd9-d8af-4287-9dcd-6d25f14c17d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences is  8551\n",
      "Number of labels is  8551\n"
     ]
    }
   ],
   "source": [
    "print ('Number of sentences is ', len(sentences))\n",
    "print ('Number of labels is ', len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qPNuwqZVK3T6"
   },
   "source": [
    "The sentences in our dataset obviously have varying lengths, so how does BERT handle this?\n",
    "\n",
    "BERT has two constraints:\n",
    "1. All sentences must be padded or truncated to a single, fixed length.\n",
    "2. The maximum sentence length is 512 tokens.\n",
    "\n",
    "Padding is done with a special `[PAD]` token, which is at index 0 in the BERT vocabulary. The below illustration demonstrates padding out to a \"MAX_LEN\" of 8 tokens.\n",
    "\n",
    "<img src=\"http://www.mccormickml.com/assets/BERT/padding_and_mask.png\" width=\"600\">\n",
    "\n",
    "The \"Attention Mask\" is simply an array of 1s and 0s indicating which tokens are padding and which aren't (seems kind of redundant, doesn't it?! Again, I don't currently know why).\n",
    "\n",
    "I've experimented with running this notebook with two different values of MAX_LEN, and it impacted both the training speed and the  test set accuracy.\n",
    "\n",
    "With a Tesla K80 and:\n",
    "\n",
    "```\n",
    "MAX_LEN = 128  -->  Training epochs take ~5:28 each, score is 0.535\n",
    "MAX_LEN = 64   -->  Training epochs take ~2:57 each, score is 0.566\n",
    "```\n",
    "These results suggest to me that the padding tokens aren't simply skipped over--that they are in fact fed through the model and incorporated in the results (thereby impacting both model speed and accuracy). I'll have to dig into the architecture more to understand this.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "TBM9ODpf3iuB",
    "outputId": "301b0bfe-f09a-4e73-fc87-d726070b7a1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of input sentence is: 231\n"
     ]
    }
   ],
   "source": [
    "print ('max length of input sentence is:',max([len(s) for s in sentences]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "OJi1yo0fzcJh",
    "outputId": "c9544ff0-0673-4e05-fa1b-4f4a056bd38d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNK] 100\n",
      "[CLS] 101\n",
      "[SEP] 102\n",
      "[MASK] 103\n"
     ]
    }
   ],
   "source": [
    "# Tokenization for BERT\n",
    "from transformers import DistilBertTokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# print out the special tokens for [CLS], [SEP] and [MASK]\n",
    "with open('distil-bert-vocab.txt','w') as f:\n",
    "    for k,v in tokenizer.vocab.items():\n",
    "        if v==100 or v==101 or v==102 or v==103:\n",
    "          print (k,v)\n",
    "\n",
    "input_sentences = []\n",
    "\n",
    "for s in sentences:\n",
    "  encoded = tokenizer.encode(s, \n",
    "                             add_special_tokens=True, \n",
    "                             max_length=64, \n",
    "                             pad_to_max_length=True)\n",
    "  input_sentences.append(encoded)\n",
    "\n",
    "assert len(input_sentences)==len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "6ivTXyDtz0fB",
    "outputId": "0e8408b5-1cfe-425c-e440-0da42d8b202a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2065, 2017, 4521, 2062, 1010, 2017, 2215, 7978, 2135, 2625, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print (input_sentences[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "_51uZty74S-7",
    "outputId": "7e1fd3b5-fd21-4036-ec40-5b1469ab98b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "8551\n",
      "8551\n",
      "8551\n"
     ]
    }
   ],
   "source": [
    "# create attention mark \n",
    "attention_masks = []\n",
    "\n",
    "for s in input_sentences:\n",
    "  a_mask = []\n",
    "  for t in s:\n",
    "    if t>0:\n",
    "      a_mask.append(1)\n",
    "    else:\n",
    "      a_mask.append(0)      \n",
    "  attention_masks.append(a_mask)\n",
    "\n",
    "print (attention_masks[0])\n",
    "print (input_sentences[0])\n",
    "\n",
    "print (len(attention_masks))\n",
    "print (len(labels))\n",
    "print (len(input_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KqE-l1AY5G77"
   },
   "outputs": [],
   "source": [
    "#split data in test and trian\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_sentences, labels, test_size=0.1, random_state=99)\n",
    "train_mask, validation_mask, _,_ = train_test_split(attention_masks, labels, random_state=99, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eWQ1eZoS5qHy"
   },
   "outputs": [],
   "source": [
    "# Convert data to Pytorch format\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "\n",
    "train_mask = torch.tensor(train_mask)\n",
    "validation_mask = torch.tensor(validation_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJKn1-tB6qXZ"
   },
   "outputs": [],
   "source": [
    "# create dataloaders for loading data in batches\n",
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset, SequentialSampler\n",
    "\n",
    "# create training dataloader\n",
    "train_data = TensorDataset(train_inputs, train_mask, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "# create validation dataloader\n",
    "validation_data = TensorDataset(validation_inputs, validation_mask, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1s8QmvyA7j-B",
    "outputId": "9537930c-d8ef-467b-a839-d6ddfedbacc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch not compiled with cuda, ignoring.\n"
     ]
    }
   ],
   "source": [
    "# Train our classification model\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',\n",
    "                                                      num_labels=2,\n",
    "                                                      output_hidden_states=False,\n",
    "                                                      output_attentions=False\n",
    "                                                      )\n",
    "try:\n",
    "    model.cuda()\n",
    "except:\n",
    "    print ('torch not compiled with cuda, ignoring.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Hfx3Mdfb978s",
    "outputId": "b3098595-59f8-4c95-f461-cfcfc79708eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of steps are: 482\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 2\n",
    "total_steps = len(train_dataloader)*epochs\n",
    "print ('Total number of steps are:', total_steps)\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5,\n",
    "                  eps = 1e-8)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=total_steps\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQUeWPrhCVZ5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3cYD-ShQMlPs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "I14HdbmH_8BC",
    "outputId": "96889036-0612-49ce-fc36-1bcc0e51cd98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "-------Training--------\n",
      "-----------------------\n",
      "Epoch 1/2\n",
      "\tBatch 0/241 in progress\n",
      "\tBatch 50/241 in progress\n",
      "\tBatch 100/241 in progress\n",
      "\tBatch 150/241 in progress\n",
      "\tBatch 200/241 in progress\n",
      "\n",
      "\tAverage training loss 0.53\n",
      "\tEpoch training time 0:18:53\n",
      "\t-----------------------\n",
      "\t-------Validation--------\n",
      "\t-----------------------\n",
      "\tValidation accuracy 0.81\n",
      "\tValidation took 0:00:37\n",
      "-----------------------\n",
      "-------Training--------\n",
      "-----------------------\n",
      "Epoch 2/2\n",
      "\tBatch 0/241 in progress\n",
      "\tBatch 50/241 in progress\n",
      "\tBatch 100/241 in progress\n",
      "\tBatch 150/241 in progress\n",
      "\tBatch 200/241 in progress\n",
      "\n",
      "\tAverage training loss 0.38\n",
      "\tEpoch training time 0:18:48\n",
      "\t-----------------------\n",
      "\t-------Validation--------\n",
      "\t-----------------------\n",
      "\tValidation accuracy 0.81\n",
      "\tValidation took 0:00:37\n",
      "Training Complete!!\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "loss_values = []\n",
    "validation_accuracy_values = []\n",
    "\n",
    "for epoch_num in range (0, epochs):\n",
    "\n",
    "  print ('-----------------------')\n",
    "  print ('-------Training--------')\n",
    "  print ('-----------------------')\n",
    "  \n",
    "  print ('==========Epoch {:}/{:}============='.format(epoch_num+1, epochs))\n",
    "  \n",
    "  t0 = time.time()\n",
    "  total_loss = 0\n",
    "\n",
    "  #put model in training mode\n",
    "  model.train()\n",
    "\n",
    "  # for each batch of training data\n",
    "  for step, batch in enumerate (train_dataloader):\n",
    "\n",
    "    # report progress after every 100 steps\n",
    "    if (step % 50==0):\n",
    "      elapsedTime = time.time()-t0\n",
    "      print ('\\tBatch {:}/{:} in progress'.format(step, len(train_dataloader)))\n",
    "\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_attention_ids = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "\n",
    "    #clear all previous gradients\n",
    "    model.zero_grad()\n",
    "\n",
    "    #we get loss in outputs\n",
    "    outputs = model(b_input_ids,\n",
    "                    attention_mask=b_attention_ids,\n",
    "                    labels=b_labels)\n",
    "    \n",
    "    loss = outputs[0]\n",
    "    total_loss+=loss.item()\n",
    "\n",
    "    #this is where backpropogation happens\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "  \n",
    "  average_loss = total_loss / len(train_dataloader)\n",
    "  loss_values.append(average_loss)\n",
    "\n",
    "  print ('')\n",
    "  print ('\\tAverage training loss {0:.2f}'.format(average_loss))\n",
    "  print ('\\tEpoch training time {:}'.format(format_time(time.time()-t0)))\n",
    "\n",
    "  print ('\\t-----------------------')\n",
    "  print ('\\t-------Validation--------')\n",
    "  print ('\\t-----------------------')\n",
    "\n",
    "  model.eval()\n",
    "  eval_accuracy = 0\n",
    "  tv0 = time.time()\n",
    "\n",
    "  for v_step, v_batch in enumerate(validation_dataloader):\n",
    "\n",
    "    b_v_input_id = v_batch[0].to(device)\n",
    "    b_v_attention_mask = v_batch[1].to(device)\n",
    "    b_v_label = v_batch[2].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      outputs = model(b_v_input_id,\n",
    "                      attention_mask=b_v_attention_mask)\n",
    "      \n",
    "    logits = outputs[0]\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_v_label.cpu().numpy()\n",
    "\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "  average_eval_accuracy = eval_accuracy / len(validation_dataloader)\n",
    "  validation_accuracy_values.append(average_eval_accuracy)\n",
    "  #print (average_eval_accuracy)\n",
    "  print ('\\tValidation accuracy {0:.2f}'.format(average_eval_accuracy))\n",
    "  print ('\\tValidation took {:}'.format(format_time(time.time()-tv0)))\n",
    "\n",
    "print ('Training Complete!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72kgDSBYAZPE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./output_dir\\\\vocab.txt',\n",
       " './output_dir\\\\special_tokens_map.json',\n",
       " './output_dir\\\\added_tokens.json')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save model to disk\n",
    "import os\n",
    "\n",
    "output_dir = './output_dir'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch not compiled with cuda, ignore\n"
     ]
    }
   ],
   "source": [
    "#load saved model from disk\n",
    "t_model = DistilBertForSequenceClassification.from_pretrained(output_dir)\n",
    "t_tokenizer = DistilBertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "try:\n",
    "    t_model.cuda()\n",
    "except:\n",
    "    print ('torch not compiled with cuda, ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0695065   0.00154585]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "test_str = ['Home was gone by John']\n",
    "for s in test_str:\n",
    "  test_input_encoded = t_tokenizer.encode(s, add_special_tokens=True, max_length=64, pad_to_max_length=True)\n",
    "\n",
    "# create attention mark \n",
    "test_attention_masks = []\n",
    "\n",
    "for t in test_input_encoded:\n",
    "  if t>0:\n",
    "    test_attention_masks.append(1)\n",
    "  else:\n",
    "    test_attention_masks.append(0)      \n",
    "\n",
    "test_input_tensor = torch.tensor(test_input_encoded).unsqueeze(0)\n",
    "test_attention_masks_tensor = torch.tensor(test_attention_masks).unsqueeze(0)\n",
    "\n",
    "test_input_tensor = test_input_tensor.to(device)\n",
    "test_attention_masks_tensor = test_attention_masks_tensor.to(device)\n",
    "\n",
    "t_model.eval()\n",
    "with torch.no_grad():\n",
    "  t_output=t_model(test_input_tensor)  \n",
    "\n",
    "logits= t_output[0]\n",
    "logits=logits.cpu().numpy()\n",
    "print (logits)\n",
    "print (np.argmax(logits,axis=1).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled22.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
