{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SAMPLES=10000\n",
    "MAX_WORDS=20000\n",
    "BATCH_SIZE=122\n",
    "EPOCHS=1\n",
    "OOV_TOKEN=0\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_SEQ_LENGTH=100\n",
    "VALIDATION_SPLIT_RATIO= 0.2\n",
    "LSTM_UNITS=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num samples is 10000\n"
     ]
    }
   ],
   "source": [
    "# Where we will store the data\n",
    "input_texts = [] # sentence in original language\n",
    "target_texts = [] # sentence in target language\n",
    "target_texts_inputs = [] # sentence in target language offset by 1\n",
    "\n",
    "t = 0\n",
    "for line in open('./data/deu.txt', encoding='utf-8'):\n",
    "    t+=1\n",
    "    if t>MAX_SAMPLES :\n",
    "        break\n",
    "        \n",
    "    input_txt, target_txt = line.rstrip().split ('\\t')\n",
    "    \n",
    "    target_txt_input = '<sos> '+target_txt\n",
    "    target_txt = target_txt +' <eos>'\n",
    "    \n",
    "    input_texts.append (input_txt)\n",
    "    target_texts_inputs.append(target_txt_input)\n",
    "    target_texts.append(target_txt)\n",
    "    \n",
    "print ('Num samples is %d'%(len(input_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max seq length of english sentences is 5\n",
      "Found 3131 unique english words\n"
     ]
    }
   ],
   "source": [
    "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_WORDS, oov_token=OOV_TOKEN, filters='')\n",
    "en_tokenizer.fit_on_texts(input_texts)\n",
    "en_word2Idx = en_tokenizer.word_index\n",
    "en_num_words = len(en_word2Idx)+1\n",
    "en_input_sequences = en_tokenizer.texts_to_sequences(input_texts)\n",
    "max_seq_len_en = min (MAX_SEQ_LENGTH, max(len(s) for s in en_input_sequences))\n",
    "padded_input_sequences = tf.keras.preprocessing.sequence.pad_sequences(en_input_sequences, padding='post', maxlen=max_seq_len_en)\n",
    "\n",
    "print ('Max seq length of english sentences is %d'%(max_seq_len_en))\n",
    "print ('Found %d unique english words'%(en_num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5042 unique german words\n",
      "Max seq length of german target sentences is 11\n",
      "Max seq length of german target input sentences is 11\n"
     ]
    }
   ],
   "source": [
    "de_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_WORDS, oov_token=OOV_TOKEN, filters='')\n",
    "de_tokenizer.fit_on_texts(target_texts+target_texts_inputs)\n",
    "de_word2Idx = de_tokenizer.word_index\n",
    "\n",
    "de_num_words = len(de_word2Idx)+1\n",
    "print ('Found %d unique german words'%(de_num_words))\n",
    "\n",
    "de_target_inputs_sequences = de_tokenizer.texts_to_sequences(target_texts_inputs) # this is for decoder input\n",
    "de_target_sequences = de_tokenizer.texts_to_sequences(target_texts) # this is for decoder output\n",
    "\n",
    "max_seq_len_target_inputs = min (MAX_SEQ_LENGTH, max(len(s) for s in de_target_inputs_sequences))\n",
    "max_seq_len_target = min (MAX_SEQ_LENGTH, max(len(s) for s in de_target_sequences))\n",
    "\n",
    "print ('Max seq length of german target sentences is %d'%(max_seq_len_target))\n",
    "print ('Max seq length of german target input sentences is %d'%(max_seq_len_target_inputs))\n",
    "\n",
    "padded_target_input_sequences = tf.keras.preprocessing.sequence.pad_sequences(de_target_inputs_sequences, padding='post', maxlen=max_seq_len_target_inputs)\n",
    "padded_target_sequences = tf.keras.preprocessing.sequence.pad_sequences(de_target_sequences, padding='post', maxlen=max_seq_len_target)\n",
    "\n",
    "assert ('<sos>' in de_word2Idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word embeddings\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "f = open ('../glove.6B.100d.txt','r', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word]=coefs\n",
    "\n",
    "f.close()\n",
    "print ('Found %s word embeddings'%(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min words to be considered are 3131\n",
      "(3131, 100)\n"
     ]
    }
   ],
   "source": [
    "num_words = min (MAX_WORDS, len(en_word2Idx)+1)\n",
    "print ('Min words to be considered are %d'%(num_words))\n",
    "\n",
    "loaded_embeddings_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in en_word2Idx.items():\n",
    "    if (i<num_words):\n",
    "        if word in embeddings_index.keys():\n",
    "            embedding_vector = embeddings_index[word]\n",
    "            loaded_embeddings_matrix[i] = embedding_vector\n",
    "\n",
    "print (loaded_embeddings_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 11, 5042)\n"
     ]
    }
   ],
   "source": [
    "one_hot_targets = np.zeros((len(padded_target_sequences), max_seq_len_target, de_num_words),dtype='float32')\n",
    "print (one_hot_targets.shape)\n",
    "for i, seq in enumerate(padded_target_sequences):\n",
    "    for j, word in enumerate(seq):\n",
    "        if (word>0):\n",
    "            one_hot_targets[i,j,word]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_layer = tf.keras.layers.Embedding(input_dim = en_num_words,\n",
    "                               output_dim = EMBEDDING_DIM,\n",
    "                               input_length=max_seq_len_en, \n",
    "                               embeddings_initializer=tf.keras.initializers.Constant(loaded_embeddings_matrix),\n",
    "                               trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0915 00:17:05.775392  8628 deprecation.py:506] From C:\\MachineLearning\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "##### build the encoder model #####\n",
    "encoder_input_ = tf.keras.layers.Input(shape=(max_seq_len_en))\n",
    "encoder_x = embed_layer(encoder_input_)\n",
    "encoder_lstm_layer_0 = tf.keras.layers.LSTM(units=LSTM_UNITS, return_sequences=True, return_state=True)\n",
    "encoder_x,h,c = encoder_lstm_layer_0(encoder_x)\n",
    "encoder_states = [h, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0915 00:17:06.301604  8628 deprecation.py:506] From C:\\MachineLearning\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "##### build the decoder model #####\n",
    "decoder_input_ = tf.keras.layers.Input(shape=(max_seq_len_target_inputs))\n",
    "target_embed_layer = tf.keras.layers.Embedding(input_dim = de_num_words,\n",
    "                               output_dim = EMBEDDING_DIM,\n",
    "                               input_length=max_seq_len_target_inputs, \n",
    "                               trainable=True)\n",
    "decoder_x = target_embed_layer(decoder_input_)\n",
    "decoder_lstm_layer_0 = tf.keras.layers.LSTM(units=LSTM_UNITS, return_sequences=True, return_state=True)\n",
    "decoder_x,_,_ = decoder_lstm_layer_0(decoder_x, initial_state=encoder_states)\n",
    "decoder_dense = tf.keras.layers.Dense(de_num_words, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 11)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 5, 100)       313100      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 11, 100)      504200      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 5, 200), (No 240800      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 11, 200), (N 240800      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 11, 5042)     1013442     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,312,342\n",
      "Trainable params: 2,312,342\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##### build the encoder-decoder model #####\n",
    "model = tf.keras.models.Model([encoder_input_, decoder_input_], decoder_outputs)\n",
    "model.compile (optimizer=tf.keras.optimizers.Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0915 00:17:22.639512  8628 deprecation.py:323] From C:\\MachineLearning\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-898b314ac064>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpadded_input_sequences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadded_target_input_sequences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVALIDATION_SPLIT_RATIO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\MachineLearning\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\MachineLearning\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    342\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mins\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[1;31m# Do not slice the training phase flag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m             \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MachineLearning\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    529\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MachineLearning\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    529\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit ([padded_input_sequences,padded_target_input_sequences], one_hot_targets, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=VALIDATION_SPLIT_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Training Metadata (Accuracy & Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-7891354b779f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0max1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m211\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epochs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "figure = plt.figure()\n",
    "\n",
    "ax1 = figure.add_subplot(211)\n",
    "ax1.plot(history.history['acc'])\n",
    "ax1.plot(history.history['val_acc'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(labels=['Training','Validation'])\n",
    "\n",
    "ax1 = figure.add_subplot(212)\n",
    "ax1.plot(history.history['loss'])\n",
    "ax1.plot(history.history['val_loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(labels=['Training','Validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LATENT_DIM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-69cf5235aa61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mencoder_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_input_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdecoder_state_input_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLATENT_DIM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdecoder_state_input_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLATENT_DIM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdecoder_states_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdecoder_state_input_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_state_input_c\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LATENT_DIM' is not defined"
     ]
    }
   ],
   "source": [
    "##### Create Encoder Model for Inference ######\n",
    "encoder_model = tf.keras.models.Model(encoder_input_, encoder_states)\n",
    "\n",
    "decoder_state_input_h = tf.keras.layers.Input(shape=(LATENT_DIM,))\n",
    "decoder_state_input_c = tf.keras.layers.Input(shape=(LATENT_DIM,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_inputs_single = tf.keras.layers.Input(shape=(1,))\n",
    "decoder_inputs_single_x = target_embed_layer(decoder_inputs_single)\n",
    "decoder_ouput_single_x, h,c = decoder_lstm_layer_0(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n",
    "decoder_states = [h, c]\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_ouput_single_x)\n",
    "\n",
    "decoder_model = tf.keras.models.Model(\n",
    "  [decoder_inputs_single, decoder_states_inputs], \n",
    "  [decoder_outputs, decoder_states]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map indexes back into real words\n",
    "# so we can view the results\n",
    "idx2word_eng = {v:k for k, v in en_word2Idx.items()}\n",
    "idx2word_trans = {v:k for k, v in de_word2Idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    encoder_states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    \n",
    "    target_seq[0, 0] = de_word2Idx['<sos>']\n",
    "    \n",
    "    output_sentence = []\n",
    "    for _ in range (len(de_target_inputs_sequences)):\n",
    "        output_tokens, h,c = decoder_model.predict([target_seq,encoder_states_value])\n",
    "        \n",
    "        # Get next word\n",
    "        idx = np.argmax(output_tokens[0, 0, :])\n",
    "        if idx == eos:\n",
    "            print ('stopping...')\n",
    "\n",
    "        word = ''\n",
    "        if idx >0 :\n",
    "            word = idx2word_trans[idx]\n",
    "            output_sentence.append(word)\n",
    "        \n",
    "        target_seq[0, 0] = idx\n",
    "            \n",
    "        encoder_states_value = [h,c]\n",
    "    \n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input - Go away!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "When using data tensors as input to a model, you should specify the `steps` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-24ad7a8b9c28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_input_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Input -'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_texts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtranslated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtest_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Translated -'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslated\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-778b1dff20fb>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Encode the input as state vectors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mencoder_states_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Generate empty target sequence of length 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MachineLearning\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[1;31m# generate symbolic tensors).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m     x, _, _ = self._standardize_user_data(\n\u001b[1;32m-> 1060\u001b[1;33m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[0;32m   1061\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MachineLearning\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2507\u001b[0m     \u001b[1;31m# Validates `steps` argument based on x's type.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2508\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcheck_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2509\u001b[1;33m       \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_steps_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2511\u001b[0m     \u001b[1;31m# First, we build/compile the model on the fly if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MachineLearning\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_steps_argument\u001b[1;34m(input_data, steps, steps_name)\u001b[0m\n\u001b[0;32m    988\u001b[0m       raise ValueError('When using {input_type} as input to a model, you should'\n\u001b[0;32m    989\u001b[0m                        ' specify the `{steps_name}` argument.'.format(\n\u001b[1;32m--> 990\u001b[1;33m                            input_type=input_type_str, steps_name=steps_name))\n\u001b[0m\u001b[0;32m    991\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: When using data tensors as input to a model, you should specify the `steps` argument."
     ]
    }
   ],
   "source": [
    "test_sentence = encoder_input_[100]\n",
    "print ('Input -', input_texts[100])\n",
    "translated = decode_sequence (test_sentence)\n",
    "print ('Translated -', translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
